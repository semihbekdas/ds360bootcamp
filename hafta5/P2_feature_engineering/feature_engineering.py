#!/usr/bin/env python3
"""
M5 Forecasting - Feature Engineering (Ã–zellik Ãœretimi)

Bu script lag ve rolling Ã¶zellikleri ile temel zaman serisi Ã¶zelliklerini Ã¼retir.
Zaman serisi tahmininde geÃ§miÅŸ deÄŸerler (lag) ve hareketli ortalamalar (rolling) 
en Ã¶nemli Ã¶zelliklerdir.

Neden Lag ve Rolling Ã–zellikler?
- Lag: GeÃ§miÅŸ satÄ±ÅŸ deÄŸerleri gelecekteki satÄ±ÅŸlarÄ± etkiler (trend, pattern)
- Rolling: KÄ±sa dÃ¶nem trendleri yakalamak iÃ§in (gÃ¼rÃ¼ltÃ¼yÃ¼ azaltÄ±r)
- Tarih: Mevsimsellik ve dÃ¶ngÃ¼sel pattern'ler iÃ§in kritik

KullanÄ±m: python feature_engineering.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime

def create_features():
    """Lag ve rolling Ã¶zellikleri ile feature engineering yap"""
    
    print("ğŸ”§ M5 Feature Engineering - Lag ve Rolling Ã–zellikler")
    print("=" * 60)
    
    # Ã‡Ä±ktÄ± klasÃ¶rlerini kontrol et
    if not os.path.exists('./artifacts/datasets'):
        print("âŒ ./artifacts/datasets/ klasÃ¶rÃ¼ bulunamadÄ±!")
        print("ğŸ’¡ Ã–nce create_m5_subset.py Ã§alÄ±ÅŸtÄ±rÄ±n")
        return
    
    # 1. Veri yÃ¼kleme
    print("\nğŸ“ 1. Train ve Validation verileri yÃ¼kleniyor...")
    
    try:
        # Train verisi
        train_df = pd.read_csv('./artifacts/datasets/train.csv', parse_dates=['date'], index_col='date')
        print(f"   âœ“ Train verisi: {train_df.shape}")
        
        # Validation verisi
        valid_df = pd.read_csv('./artifacts/datasets/valid.csv', parse_dates=['date'], index_col='date')
        print(f"   âœ“ Valid verisi: {valid_df.shape}")
        
        # Veri tiplerini kontrol et
        print(f"   â€¢ Train tarih aralÄ±ÄŸÄ±: {train_df.index.min()} - {train_df.index.max()}")
        print(f"   â€¢ Valid tarih aralÄ±ÄŸÄ±: {valid_df.index.min()} - {valid_df.index.max()}")
        
    except FileNotFoundError as e:
        print(f"âŒ Dosya bulunamadÄ±: {e}")
        print("ğŸ’¡ Ã–nce create_m5_subset.py Ã§alÄ±ÅŸtÄ±rÄ±n")
        return
    
    # Veriyi birleÅŸtir (feature engineering iÃ§in tam zaman serisi gerekli)
    print(f"\nğŸ”— 2. Train ve Valid birleÅŸtiriliyor (FE iÃ§in tam seri gerekli)...")
    
    # BirleÅŸtir ve sÄ±rala
    all_df = pd.concat([train_df, valid_df]).sort_index()
    print(f"   âœ“ BirleÅŸik veri: {all_df.shape}")
    print(f"   â€¢ Toplam tarih aralÄ±ÄŸÄ±: {all_df.index.min()} - {all_df.index.max()}")
    
    # Her Ã¼rÃ¼n iÃ§in ayrÄ± feature engineering
    print(f"\nâš™ï¸ 3. Feature Engineering baÅŸlÄ±yor...")
    
    feature_data = []
    
    for item_id in all_df['item_id'].unique():
        print(f"   â€¢ Ä°ÅŸleniyor: {item_id}")
        
        # ÃœrÃ¼n verisini al
        item_df = all_df[all_df['item_id'] == item_id].copy()
        item_df = item_df.sort_index()  # Tarih sÄ±ralamasÄ± Ã¶nemli
        
        # ===================
        # LAG Ã–ZELLÄ°KLERÄ°
        # ===================
        # Neden lag? GeÃ§miÅŸ satÄ±ÅŸ deÄŸerleri gelecekteki satÄ±ÅŸlarÄ±n en gÃ¼Ã§lÃ¼ gÃ¶stergesidir.
        # - lag_1: DÃ¼n ne sattÄ±k? (kÄ±sa dÃ¶nem trend)
        # - lag_7: 1 hafta Ã¶nce ne sattÄ±k? (haftalÄ±k pattern)
        # - lag_28: 4 hafta Ã¶nce ne sattÄ±k? (aylÄ±k pattern)
        
        print(f"     â†’ Lag Ã¶zellikleri ekleniyor...")
        item_df['lag_1'] = item_df['sales'].shift(1)    # 1 gÃ¼n Ã¶nce
        item_df['lag_7'] = item_df['sales'].shift(7)    # 1 hafta Ã¶nce
        item_df['lag_28'] = item_df['sales'].shift(28)  # 4 hafta Ã¶nce
        
        # ===================
        # ROLLING Ã–ZELLÄ°KLERÄ°
        # ===================
        # Neden rolling? Ham veriler gÃ¼rÃ¼ltÃ¼lÃ¼ olabilir, hareketli ortalama trend'i yakalÄ±yor.
        # - roll_mean_7: Son 1 haftanÄ±n ortalamasÄ± (kÄ±sa dÃ¶nem trend)
        # - roll_mean_28: Son 4 haftanÄ±n ortalamasÄ± (orta dÃ¶nem trend)
        # min_periods=1: Ä°lk gÃ¼nlerde bile hesaplama yap
        
        print(f"     â†’ Rolling Ã¶zellikler ekleniyor...")
        item_df['roll_mean_7'] = item_df['sales'].rolling(window=7, min_periods=1).mean()
        item_df['roll_mean_28'] = item_df['sales'].rolling(window=28, min_periods=1).mean()
        
        # ===================
        # TARÄ°H Ã–ZELLÄ°KLERÄ°
        # ===================
        # Neden tarih Ã¶zellikleri? Mevsimsellik ve dÃ¶ngÃ¼sel pattern'ler kritik.
        # - dow: HaftanÄ±n gÃ¼nÃ¼ (0=Pazartesi, 6=Pazar) - hafta sonu/iÃ§i farkÄ±
        # - dom: AyÄ±n gÃ¼nÃ¼ (1-31) - ay baÅŸÄ±/sonu farkÄ±  
        # - weekofyear: YÄ±lÄ±n haftasÄ± (1-53) - yÄ±llÄ±k trend
        # - month: Ay (1-12) - mevsimsel pattern
        
        print(f"     â†’ Tarih Ã¶zellikleri ekleniyor...")
        item_df['dow'] = item_df.index.dayofweek        # 0-6 (Pazartesi-Pazar)
        item_df['dom'] = item_df.index.day              # 1-31
        item_df['weekofyear'] = item_df.index.isocalendar().week  # 1-53
        item_df['month'] = item_df.index.month          # 1-12
        
        # Store ve item bilgilerini koru
        item_df['item_id'] = item_id
        item_df['store_id'] = item_df['store_id'].iloc[0]
        
        feature_data.append(item_df)
    
    # TÃ¼m Ã¼rÃ¼nleri birleÅŸtir
    feature_df = pd.concat(feature_data, ignore_index=False)
    feature_df = feature_df.sort_index()
    
    print(f"   âœ“ Feature engineering tamamlandÄ±: {feature_df.shape}")
    
    # 4. NaN deÄŸerleri handle et
    print(f"\nğŸ”§ 4. NaN deÄŸerleri kontrol ediliyor ve dolduruluyor...")
    
    # NaN istatistikleri
    nan_counts = feature_df.isnull().sum()
    nan_features = nan_counts[nan_counts > 0]
    
    if len(nan_features) > 0:
        print(f"   â€¢ NaN olan Ã¶zellikler:")
        for feature, count in nan_features.items():
            print(f"     - {feature}: {count} NaN ({count/len(feature_df)*100:.1f}%)")
    else:
        print(f"   âœ“ HiÃ§ NaN deÄŸer yok!")
    
    # NaN doldurma stratejisi
    print(f"   â€¢ NaN doldurma uygulanÄ±yor...")
    
    # Lag Ã¶zellikleri: BaÅŸlangÄ±Ã§ta NaN normal (geÃ§miÅŸ veri yok)
    # Ä°lk deÄŸerleri 0 ile doldur (muhafazakar yaklaÅŸÄ±m)
    lag_features = ['lag_1', 'lag_7', 'lag_28']
    for lag_col in lag_features:
        if lag_col in feature_df.columns:
            before_count = feature_df[lag_col].isnull().sum()
            feature_df[lag_col] = feature_df[lag_col].fillna(0)
            after_count = feature_df[lag_col].isnull().sum()
            print(f"     - {lag_col}: {before_count} â†’ {after_count} NaN")
    
    # Rolling Ã¶zellikler: min_periods=1 kullandÄ±k, NaN olmamalÄ±
    roll_features = ['roll_mean_7', 'roll_mean_28']
    for roll_col in roll_features:
        if roll_col in feature_df.columns:
            before_count = feature_df[roll_col].isnull().sum()
            if before_count > 0:
                feature_df[roll_col] = feature_df[roll_col].fillna(method='ffill').fillna(0)
                after_count = feature_df[roll_col].isnull().sum()
                print(f"     - {roll_col}: {before_count} â†’ {after_count} NaN")
    
    # Final kontrol
    final_nan = feature_df.isnull().sum().sum()
    print(f"   âœ“ Final NaN sayÄ±sÄ±: {final_nan}")
    
    # 5. Train/Valid'e tekrar bÃ¶l
    print(f"\nâœ‚ï¸ 5. Train/Validation'a tekrar bÃ¶lÃ¼nÃ¼yor...")
    
    # Orijinal split tarihini bul
    train_end_date = train_df.index.max()
    valid_start_date = valid_df.index.min()
    
    print(f"   â€¢ Train son tarihi: {train_end_date}")
    print(f"   â€¢ Valid ilk tarihi: {valid_start_date}")
    
    # Feature'lÄ± veriyi bÃ¶l
    fe_train = feature_df[feature_df.index <= train_end_date].copy()
    fe_valid = feature_df[feature_df.index >= valid_start_date].copy()
    
    print(f"   âœ“ FE Train: {fe_train.shape}")
    print(f"   âœ“ FE Valid: {fe_valid.shape}")
    
    # 6. X, y'ye ayÄ±r
    print(f"\nğŸ¯ 6. Ã–zellik matrisi (X) ve hedef (y) ayrÄ±lÄ±yor...")
    
    # Hedef deÄŸiÅŸken
    target_col = 'sales'
    
    # Ã–zellik sÃ¼tunlarÄ± (sales, item_id, store_id hariÃ§)
    feature_cols = [col for col in fe_train.columns 
                   if col not in [target_col, 'item_id', 'store_id']]
    
    print(f"   â€¢ Ã–zellik sayÄ±sÄ±: {len(feature_cols)}")
    print(f"   â€¢ Ã–zellikler: {feature_cols}")
    
    # Train set
    X_train = fe_train[feature_cols].copy()
    y_train = fe_train[target_col].copy()
    
    # Valid set
    X_valid = fe_valid[feature_cols].copy()
    y_valid = fe_valid[target_col].copy()
    
    print(f"   âœ“ X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"   âœ“ X_valid: {X_valid.shape}, y_valid: {y_valid.shape}")
    
    # Metadata'yÄ± ayrÄ± tut (isteÄŸe baÄŸlÄ±)
    train_meta = fe_train[['item_id', 'store_id']].copy()
    valid_meta = fe_valid[['item_id', 'store_id']].copy()
    
    # 7. Parquet olarak kaydet
    print(f"\nğŸ’¾ 7. Feature engineered veriler kaydediliyor...")
    
    # Tam feature dataset'leri kaydet (meta bilgilerle)
    fe_train_path = './artifacts/datasets/fe_train.parquet'
    fe_valid_path = './artifacts/datasets/fe_valid.parquet'
    
    fe_train.to_parquet(fe_train_path)
    fe_valid.to_parquet(fe_valid_path)
    
    print(f"   âœ“ FE Train: {fe_train_path}")
    print(f"   âœ“ FE Valid: {fe_valid_path}")
    
    # X, y matrislerini de kaydet (model iÃ§in direkt kullanÄ±m)
    X_train.to_parquet('./artifacts/datasets/X_train.parquet')
    y_train.to_frame('sales').to_parquet('./artifacts/datasets/y_train.parquet')
    X_valid.to_parquet('./artifacts/datasets/X_valid.parquet')
    y_valid.to_frame('sales').to_parquet('./artifacts/datasets/y_valid.parquet')
    
    print(f"   âœ“ X_train, y_train, X_valid, y_valid kaydedildi")
    
    # 8. Ã–zellik analizi
    print(f"\nğŸ“Š 8. Ã–zellik analizi yapÄ±lÄ±yor...")
    
    # Describe istatistikleri
    print(f"\nğŸ“ˆ Ã–ZELLÄ°K Ä°STATÄ°STÄ°KLERÄ°:")
    print("=" * 50)
    
    feature_stats = X_train.describe()
    print(feature_stats.round(2))
    
    # Korelasyon analizi
    print(f"\nğŸ”— HEDEF Ä°LE KORELASYON:")
    print("=" * 30)
    
    # Training setinde hedef ile korelasyon
    corr_with_target = X_train.corrwith(y_train).sort_values(ascending=False)
    
    print("En yÃ¼ksek korelasyonlu Ã¶zellikler:")
    for feature, corr in corr_with_target.items():
        print(f"  {feature:15}: {corr:6.3f}")
    
    # 9. GÃ¶rselleÅŸtirme
    print(f"\nğŸ“Š 9. Ã–zellik daÄŸÄ±lÄ±mlarÄ± gÃ¶rselleÅŸtiriliyor...")
    
    # Ã–zellik histogramlarÄ±
    n_features = len(feature_cols)
    n_cols = 3
    n_rows = (n_features + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
    axes = axes.flatten() if n_rows > 1 else [axes] if n_features == 1 else axes
    
    for i, feature in enumerate(feature_cols):
        if i < len(axes):
            ax = axes[i]
            
            # Histogram
            X_train[feature].hist(bins=30, alpha=0.7, ax=ax)
            ax.set_title(f'{feature}\nMean: {X_train[feature].mean():.2f}')
            ax.set_ylabel('Frequency')
            ax.grid(True, alpha=0.3)
    
    # BoÅŸ subplot'larÄ± gizle
    for i in range(n_features, len(axes)):
        axes[i].set_visible(False)
    
    plt.suptitle('Feature Distributions (Training Set)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # Kaydet
    hist_path = './artifacts/figures/feature_distributions.png'
    plt.savefig(hist_path, dpi=300, bbox_inches='tight')
    print(f"   âœ“ Feature histogramlarÄ±: {hist_path}")
    plt.close()
    
    # Korelasyon Ä±sÄ± haritasÄ±
    plt.figure(figsize=(10, 8))
    
    # TÃ¼m Ã¶zelliklerin birbirleriyle korelasyonu
    correlation_matrix = X_train.corr()
    
    # Heatmap
    mask = np.triu(correlation_matrix)  # Ãœst Ã¼Ã§geni gizle
    sns.heatmap(correlation_matrix, 
                mask=mask,
                annot=True, 
                cmap='coolwarm', 
                center=0,
                square=True,
                fmt='.2f',
                cbar_kws={'label': 'Correlation'})
    
    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # Kaydet
    corr_path = './artifacts/figures/feature_correlations.png'
    plt.savefig(corr_path, dpi=300, bbox_inches='tight')
    print(f"   âœ“ Korelasyon matrisi: {corr_path}")
    plt.close()
    
    # 10. Ã–zet rapor
    print(f"\nğŸ“‹ FEATURE ENGINEERÄ°NG Ã–ZETÄ°")
    print("=" * 50)
    print(f"â€¢ Toplam Ã¶zellik sayÄ±sÄ±: {len(feature_cols)}")
    print(f"â€¢ Lag Ã¶zellikleri: {len([f for f in feature_cols if 'lag' in f])}")
    print(f"â€¢ Rolling Ã¶zellikleri: {len([f for f in feature_cols if 'roll' in f])}")
    print(f"â€¢ Tarih Ã¶zellikleri: {len([f for f in feature_cols if f in ['dow', 'dom', 'weekofyear', 'month']])}")
    print(f"â€¢ Train Ã¶rnekleri: {len(X_train):,}")
    print(f"â€¢ Valid Ã¶rnekleri: {len(X_valid):,}")
    print(f"â€¢ Hedef ortalamasÄ± (train): {y_train.mean():.2f}")
    print(f"â€¢ Hedef std (train): {y_train.std():.2f}")
    
    print(f"\nğŸ¯ EN Ã–NEMLÄ° Ã–ZELLÄ°KLER (korelasyon bazÄ±nda):")
    top_features = corr_with_target.abs().nlargest(5)
    for i, (feature, corr) in enumerate(top_features.items(), 1):
        print(f"  {i}. {feature}: {corr:.3f}")
    
    print(f"\nâœ… Feature Engineering tamamlandÄ±!")
    print(f"ğŸ“ Ã‡Ä±ktÄ±lar: ./artifacts/datasets/ ve ./artifacts/figures/")
    
    return fe_train, fe_valid, X_train, y_train, X_valid, y_valid

if __name__ == "__main__":
    try:
        print("ğŸš€ M5 Feature Engineering baÅŸlatÄ±lÄ±yor...")
        
        fe_train, fe_valid, X_train, y_train, X_valid, y_valid = create_features()
        
        print(f"\nğŸ‰ Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
        print(f"ğŸ“Š ArtÄ±k makine Ã¶ÄŸrenmesi modellerini eÄŸitebilirsiniz.")
        
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()