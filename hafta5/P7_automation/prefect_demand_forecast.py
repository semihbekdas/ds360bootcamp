#!/usr/bin/env python3
"""
Prefect Demand Forecasting Pipeline

Bu script her sabah 09:00 Europe/Istanbul saatinde Ã§alÄ±ÅŸacak otomatik talep tahmin pipeline'Ä±:
1. Veri yÃ¼kleme
2. Feature engineering
3. Model eÄŸitimi/yÃ¼kleme
4. Tahmin Ã¼retme
5. SonuÃ§larÄ± kaydetme

Prefect Schedule:
- Cron: "0 9 * * *" Europe/Istanbul
- Her gÃ¼n sabah 9'da Ã§alÄ±ÅŸÄ±r
- Production-ready workflow orchestration

KullanÄ±m:
python prefect_demand_forecast.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import os
import pickle
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional

# Prefect
try:
    from prefect import task, flow
    PREFECT_AVAILABLE = True
except ImportError:
    print("âš ï¸  Prefect bulunamadÄ±. Normal fonksiyonlar kullanÄ±lacak.")
    PREFECT_AVAILABLE = False
    
    # Mock decorators for non-Prefect environments
    def task(func):
        return func
    def flow(func):
        return func

# LightGBM
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    print("âŒ LightGBM kÃ¼tÃ¼phanesi bulunamadÄ±.")
    LIGHTGBM_AVAILABLE = False

# Preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error

warnings.filterwarnings('ignore')

# ================================
# PREFECT TASKS
# ================================

@task
def load_data_task(artifacts_path: str = "/Users/yaseminarslan/Desktop/ds360_ikincihafta/hafta5/artifacts") -> pd.DataFrame:
    """
    Veri yÃ¼kleme gÃ¶revi (P1'in basitleÅŸtirilmiÅŸ hali)
    
    GerÃ§ek production'da:
    - Database connection
    - API calls
    - Data validation
    """
    print("ğŸ“ Veri yÃ¼kleme baÅŸlÄ±yor...")
    
    try:
        # Mevcut feature engineered veriyi yÃ¼kle
        train_path = f'{artifacts_path}/datasets/fe_train.parquet'
        valid_path = f'{artifacts_path}/datasets/fe_valid.parquet'
        
        if os.path.exists(train_path) and os.path.exists(valid_path):
            train_df = pd.read_parquet(train_path)
            valid_df = pd.read_parquet(valid_path)
            full_df = pd.concat([train_df, valid_df]).sort_index()
            
            print(f"   âœ“ Veri yÃ¼klendi: {full_df.shape}")
            print(f"   â€¢ Tarih aralÄ±ÄŸÄ±: {full_df.index.min()} - {full_df.index.max()}")
            print(f"   â€¢ ÃœrÃ¼n sayÄ±sÄ±: {full_df['item_id'].nunique()}")
            
            return full_df
        else:
            raise FileNotFoundError("Feature engineered veriler bulunamadÄ±. Ã–nce Ã¶nceki scriptleri Ã§alÄ±ÅŸtÄ±rÄ±n.")
            
    except Exception as e:
        print(f"   âŒ Veri yÃ¼kleme hatasÄ±: {e}")
        raise

@task
def feature_engineer_task(data_df: pd.DataFrame) -> pd.DataFrame:
    """
    Feature engineering gÃ¶revi (P2'den gerekli kÄ±sÄ±m)
    
    Not: Bu Ã¶rnekte veri zaten feature engineered, 
    ama production'da fresh data iÃ§in gerekli
    """
    print("âš™ï¸ Feature engineering baÅŸlÄ±yor...")
    
    try:
        # Veri zaten FE edilmiÅŸ ama son kontroller
        processed_df = data_df.copy()
        
        # Eksik deÄŸer kontrolÃ¼
        missing_counts = processed_df.isnull().sum()
        if missing_counts.sum() > 0:
            print(f"   âš ï¸  {missing_counts.sum()} eksik deÄŸer bulundu, doldurulacak")
            # Lag ve rolling features iÃ§in forward fill
            processed_df = processed_df.fillna(method='ffill').fillna(0)
        
        # Feature validation
        required_features = ['lag_1', 'lag_7', 'lag_28', 'roll_mean_7', 'roll_mean_28', 
                           'dow', 'dom', 'weekofyear', 'month', 'item_id', 'store_id', 'sales']
        
        missing_features = [f for f in required_features if f not in processed_df.columns]
        if missing_features:
            raise ValueError(f"Eksik Ã¶zellikler: {missing_features}")
        
        print(f"   âœ“ Feature engineering tamamlandÄ±: {processed_df.shape}")
        print(f"   â€¢ Ã–zellik sayÄ±sÄ±: {len(processed_df.columns)}")
        
        return processed_df
        
    except Exception as e:
        print(f"   âŒ Feature engineering hatasÄ±: {e}")
        raise

@task
def train_or_load_model_task(data_df: pd.DataFrame, 
                            artifacts_path: str = "./artifacts") -> Dict:
    """
    Model eÄŸitimi veya yÃ¼kleme gÃ¶revi
    
    EÄŸer model dosyasÄ± varsa yÃ¼kle, yoksa eÄŸit
    """
    print("ğŸ¤– Model eÄŸitimi/yÃ¼kleme baÅŸlÄ±yor...")
    
    model_path = f'{artifacts_path}/models/lgbm.pkl'
    
    try:
        # Model dosyasÄ± var mÄ± kontrol et
        if os.path.exists(model_path):
            print("   ğŸ“‚ Mevcut model yÃ¼kleniyor...")
            
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            model = model_data['model']
            feature_cols = model_data['feature_cols']
            label_encoders = model_data['label_encoders']
            
            print(f"   âœ“ Model yÃ¼klendi: LightGBM")
            print(f"   â€¢ Ã–zellik sayÄ±sÄ±: {len(feature_cols)}")
            print(f"   â€¢ Model iterasyonu: {model.best_iteration}")
            
            return {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'is_new_model': False
            }
            
        else:
            print("   ğŸ¯ Yeni model eÄŸitiliyor...")
            
            # Kategorik encoding
            label_encoders = {}
            data_encoded = data_df.copy()
            
            for col in ['item_id', 'store_id']:
                if col in data_encoded.columns:
                    le = LabelEncoder()
                    data_encoded[f'{col}_encoded'] = le.fit_transform(data_encoded[col])
                    label_encoders[col] = le
            
            # Feature/target ayÄ±rma
            feature_cols = [col for col in data_encoded.columns if col in [
                'lag_1', 'lag_7', 'lag_28', 'roll_mean_7', 'roll_mean_28',
                'dow', 'dom', 'weekofyear', 'month', 'item_id_encoded', 'store_id_encoded'
            ]]
            
            X = data_encoded[feature_cols].fillna(0)
            y = data_encoded['sales']
            
            # Train/validation split (son %10 validation)
            split_idx = int(len(X) * 0.9)
            X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            # LightGBM parametreleri
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'learning_rate': 0.05,
                'num_leaves': 31,
                'verbose': -1,
                'random_state': 42
            }
            
            # Model eÄŸitimi
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            
            model = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=500,
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            # Model kaydet
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            model_data = {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'training_date': datetime.now().isoformat()
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            print(f"   âœ“ Model eÄŸitimi tamamlandÄ± ve kaydedildi")
            print(f"   â€¢ Best iteration: {model.best_iteration}")
            
            return {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'is_new_model': True
            }
            
    except Exception as e:
        print(f"   âŒ Model eÄŸitimi/yÃ¼kleme hatasÄ±: {e}")
        raise

@task
def predict_task(data_df: pd.DataFrame, 
                model_info: Dict, 
                forecast_days: int = 7) -> pd.DataFrame:
    """
    Tahmin Ã¼retme gÃ¶revi
    
    Son gÃ¼ne kadar gÃ¼ncelle ve +7 gÃ¼n (veya +28) tahmin Ã¼ret
    """
    print(f"ğŸ”® {forecast_days} gÃ¼nlÃ¼k tahmin Ã¼retiliyor...")
    
    try:
        model = model_info['model']
        feature_cols = model_info['feature_cols']
        label_encoders = model_info['label_encoders']
        
        # Son tarihi bul
        last_date = data_df.index.max()
        forecast_start = last_date + timedelta(days=1)
        
        print(f"   â€¢ Son veri tarihi: {last_date}")
        print(f"   â€¢ Tahmin baÅŸlangÄ±cÄ±: {forecast_start}")
        
        # Her Ã¼rÃ¼n iÃ§in tahmin
        all_predictions = []
        unique_items = data_df['item_id'].unique()
        
        for item_id in unique_items:
            # ÃœrÃ¼n verisini al
            item_data = data_df[data_df['item_id'] == item_id].copy()
            
            if len(item_data) == 0:
                continue
            
            # Son satÄ±rÄ± template olarak kullan
            last_row = item_data.iloc[-1].copy()
            
            # Kategorik encoding
            for col, le in label_encoders.items():
                if col in last_row:
                    try:
                        last_row[f'{col}_encoded'] = le.transform([last_row[col]])[0]
                    except ValueError:
                        last_row[f'{col}_encoded'] = 0  # Unseen value
            
            # Her gÃ¼n iÃ§in tahmin
            for day in range(forecast_days):
                forecast_date = forecast_start + timedelta(days=day)
                
                # Tarih Ã¶zelliklerini gÃ¼ncelle
                current_features = last_row.copy()
                current_features['dow'] = forecast_date.weekday()
                current_features['dom'] = forecast_date.day
                current_features['weekofyear'] = forecast_date.isocalendar()[1]
                current_features['month'] = forecast_date.month
                
                # Tahmin yap
                X_pred = current_features[feature_cols].values.reshape(1, -1)
                y_pred = model.predict(X_pred, num_iteration=model.best_iteration)[0]
                y_pred = max(0, y_pred)  # Negatif deÄŸerleri 0 yap
                
                # Sonucu kaydet
                all_predictions.append({
                    'date': forecast_date,
                    'item_id': item_id,
                    'store_id': last_row['store_id'],
                    'predicted_sales': y_pred,
                    'model_type': 'LightGBM',
                    'forecast_horizon': day + 1
                })
        
        # DataFrame'e Ã§evir
        predictions_df = pd.DataFrame(all_predictions)
        
        print(f"   âœ“ {len(predictions_df)} tahmin Ã¼retildi")
        print(f"   â€¢ ÃœrÃ¼n sayÄ±sÄ±: {predictions_df['item_id'].nunique()}")
        print(f"   â€¢ Ortalama tahmin: {predictions_df['predicted_sales'].mean():.2f}")
        
        return predictions_df
        
    except Exception as e:
        print(f"   âŒ Tahmin hatasÄ±: {e}")
        raise

@task
def save_outputs_task(predictions_df: pd.DataFrame, 
                     run_date: str,
                     artifacts_path: str = "./artifacts") -> Dict[str, str]:
    """
    SonuÃ§larÄ± kaydetme gÃ¶revi
    
    CSV ve PNG dosyalarÄ±nÄ± ./artifacts/preds/ klasÃ¶rÃ¼ne kaydet
    """
    print("ğŸ’¾ SonuÃ§lar kaydediliyor...")
    
    try:
        # Ã‡Ä±ktÄ± klasÃ¶rÃ¼
        output_dir = f'{artifacts_path}/preds'
        os.makedirs(output_dir, exist_ok=True)
        
        # Dosya adlarÄ±
        run_date_str = datetime.strptime(run_date, '%Y-%m-%d').strftime('%Y%m%d')
        csv_path = f'{output_dir}/run_{run_date_str}.csv'
        png_path = f'{output_dir}/run_{run_date_str}_summary.png'
        
        # 1. CSV kaydet
        predictions_df.to_csv(csv_path, index=False)
        print(f"   âœ“ CSV kaydedildi: {csv_path}")
        
        # 2. Ã–zet grafik oluÅŸtur
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # GÃ¼nlÃ¼k toplam tahmin
        daily_total = predictions_df.groupby('date')['predicted_sales'].sum()
        axes[0, 0].plot(daily_total.index, daily_total.values, marker='o', linewidth=2)
        axes[0, 0].set_title('GÃ¼nlÃ¼k Toplam Tahmin')
        axes[0, 0].set_ylabel('Toplam SatÄ±ÅŸ')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ÃœrÃ¼n bazÄ±nda toplam
        item_totals = predictions_df.groupby('item_id')['predicted_sales'].sum().sort_values(ascending=False)
        axes[0, 1].bar(range(len(item_totals)), item_totals.values, alpha=0.7)
        axes[0, 1].set_title('ÃœrÃ¼n BazÄ±nda Toplam Tahmin')
        axes[0, 1].set_ylabel('Toplam SatÄ±ÅŸ')
        axes[0, 1].set_xticks(range(len(item_totals)))
        axes[0, 1].set_xticklabels(item_totals.index, rotation=45)
        
        # Forecast horizon analizi
        horizon_avg = predictions_df.groupby('forecast_horizon')['predicted_sales'].mean()
        axes[1, 0].plot(horizon_avg.index, horizon_avg.values, marker='s', linewidth=2)
        axes[1, 0].set_title('Tahmin Ufku Analizi')
        axes[1, 0].set_xlabel('GÃ¼n')
        axes[1, 0].set_ylabel('Ortalama SatÄ±ÅŸ')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Ã–zet istatistikler
        axes[1, 1].axis('off')
        stats_text = f"""
Tahmin Ã–zeti ({run_date})

â€¢ Toplam tahmin: {len(predictions_df):,}
â€¢ ÃœrÃ¼n sayÄ±sÄ±: {predictions_df['item_id'].nunique()}
â€¢ Forecast horizon: {predictions_df['forecast_horizon'].max()} gÃ¼n
â€¢ Ortalama gÃ¼nlÃ¼k satÄ±ÅŸ: {predictions_df['predicted_sales'].mean():.1f}
â€¢ Toplam beklenen satÄ±ÅŸ: {predictions_df['predicted_sales'].sum():.0f}
â€¢ Min tahmin: {predictions_df['predicted_sales'].min():.1f}
â€¢ Max tahmin: {predictions_df['predicted_sales'].max():.1f}

Model: LightGBM
Pipeline: Prefect Automated
Ã‡alÄ±ÅŸma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """
        axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=11, verticalalignment='top', 
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.suptitle(f'Demand Forecast Summary - {run_date}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # PNG kaydet
        plt.savefig(png_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ“ PNG kaydedildi: {png_path}")
        
        # SonuÃ§ Ã¶zeti
        summary = {
            'csv_path': csv_path,
            'png_path': png_path,
            'total_predictions': len(predictions_df),
            'n_items': int(predictions_df['item_id'].nunique()),
            'forecast_days': int(predictions_df['forecast_horizon'].max()),
            'total_expected_sales': float(predictions_df['predicted_sales'].sum()),
            'avg_daily_sales': float(predictions_df['predicted_sales'].mean())
        }
        
        print(f"   âœ“ Ã‡Ä±ktÄ±lar baÅŸarÄ±yla kaydedildi")
        
        return summary
        
    except Exception as e:
        print(f"   âŒ Ã‡Ä±ktÄ± kaydetme hatasÄ±: {e}")
        raise

# ================================
# PREFECT FLOW
# ================================

@flow
def demand_forecast_flow(run_date: Optional[str] = None, 
                        forecast_days: int = 7,
                        artifacts_path: str = "./artifacts") -> Dict:
    """
    Ana demand forecasting flow
    
    Parameters:
    - run_date: Ã‡alÄ±ÅŸma tarihi (YYYY-MM-DD format, default: bugÃ¼n)
    - forecast_days: KaÃ§ gÃ¼n tahmin (default: 7)
    - artifacts_path: Artifacts klasÃ¶r yolu
    
    Schedule: Cron "0 9 * * *" Europe/Istanbul
    - Her gÃ¼n sabah 09:00'da Ã§alÄ±ÅŸÄ±r
    - Ä°stanbul saati ile (TÃ¼rkiye timezone)
    - Production ortamÄ±nda Prefect server/cloud ile yÃ¶netilir
    """
    
    # Default run date
    if run_date is None:
        run_date = datetime.now().strftime('%Y-%m-%d')
    
    print("ğŸª DEMAND FORECAST PIPELINE BAÅLIYOR")
    print("=" * 50)
    print(f"ğŸ“… Ã‡alÄ±ÅŸma tarihi: {run_date}")
    print(f"ğŸ”® Forecast horizon: {forecast_days} gÃ¼n")
    print(f"ğŸ“ Artifacts path: {artifacts_path}")
    print(f"â° BaÅŸlangÄ±Ã§ zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if PREFECT_AVAILABLE:
        print("âœ… Prefect modu: Task orchestration aktif")
    else:
        print("âš ï¸  Normal mod: Sequential execution")
    
    print("-" * 50)
    
    try:
        # 1. Veri yÃ¼kleme
        data_df = load_data_task(artifacts_path)
        
        # 2. Feature engineering
        processed_df = feature_engineer_task(data_df)
        
        # 3. Model eÄŸitimi/yÃ¼kleme
        model_info = train_or_load_model_task(processed_df, artifacts_path)
        
        # 4. Tahmin Ã¼retme
        predictions_df = predict_task(processed_df, model_info, forecast_days)
        
        # 5. SonuÃ§larÄ± kaydetme
        output_summary = save_outputs_task(predictions_df, run_date, artifacts_path)
        
        # Pipeline Ã¶zeti
        pipeline_summary = {
            'run_date': run_date,
            'forecast_days': forecast_days,
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_shape': list(data_df.shape),
            'model_info': {
                'type': 'LightGBM',
                'is_new_model': model_info['is_new_model'],
                'feature_count': len(model_info['feature_cols'])
            },
            'output_summary': output_summary,
            'status': 'SUCCESS'
        }
        
        print("-" * 50)
        print("ğŸ‰ PIPELINE BAÅARIYLA TAMAMLANDI!")
        print(f"ğŸ“Š Toplam tahmin: {output_summary['total_predictions']}")
        print(f"ğŸ¯ ÃœrÃ¼n sayÄ±sÄ±: {output_summary['n_items']}")
        print(f"ğŸ’° Beklenen toplam satÄ±ÅŸ: {output_summary['total_expected_sales']:.0f}")
        print(f"ğŸ“ CSV: {output_summary['csv_path']}")
        print(f"ğŸ“ˆ PNG: {output_summary['png_path']}")
        print(f"â° BitiÅŸ zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return pipeline_summary
        
    except Exception as e:
        print("-" * 50)
        print(f"âŒ PIPELINE HATASI: {e}")
        
        error_summary = {
            'run_date': run_date,
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'error': str(e),
            'status': 'FAILED'
        }
        
        raise

# ================================
# SCHEDULING & MAIN
# ================================

def setup_schedule():
    """
    Prefect schedule setup (eÄŸitim amaÃ§lÄ± gÃ¶sterim)
    
    GerÃ§ek production iÃ§in:
    1. prefect deployment build prefect_demand_forecast.py:demand_forecast_flow -n "daily-forecast"
    2. prefect deployment apply demand_forecast_flow-deployment.yaml
    3. prefect agent start -q default
    """
    
    if not PREFECT_AVAILABLE:
        print("âš ï¸  Prefect mevcut deÄŸil, schedule setup atlanÄ±yor")
        return
    
    try:
        from prefect.deployments import Deployment
        from prefect.server.schemas.schedules import CronSchedule
        
        # Cron schedule: Her gÃ¼n 09:00 Europe/Istanbul
        schedule = CronSchedule(
            cron="0 9 * * *",  # Dakika Saat GÃ¼n Ay HaftanÄ±n-gÃ¼nÃ¼
            timezone="Europe/Istanbul"
        )
        
        deployment = Deployment.build_from_flow(
            flow=demand_forecast_flow,
            name="daily-demand-forecast",
            description="Her sabah 09:00'da Ã§alÄ±ÅŸan otomatik talep tahmin pipeline'Ä±",
            schedule=schedule,
            parameters={
                "forecast_days": 7,
                "artifacts_path": "./artifacts"
            },
            tags=["production", "forecasting", "daily"]
        )
        
        print("ğŸ“… Prefect Deployment hazÄ±rlandÄ±:")
        print(f"   â€¢ Schedule: 0 9 * * * (her gÃ¼n 09:00)")
        print(f"   â€¢ Timezone: Europe/Istanbul")
        print(f"   â€¢ Name: daily-demand-forecast")
        
        # Deployment'Ä± apply etmek iÃ§in:
        # deployment.apply()
        
        return deployment
        
    except Exception as e:
        print(f"âš ï¸  Schedule setup hatasÄ±: {e}")
        return None

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    
    print("ğŸš€ PREFECT DEMAND FORECASTING PIPELINE")
    print("=" * 60)
    
    if not LIGHTGBM_AVAILABLE:
        print("âŒ LightGBM gerekli")
        return
    
    print("ğŸ“‹ Pipeline Bilgileri:")
    print("   â€¢ Schedule: Her gÃ¼n 09:00 Europe/Istanbul")
    print("   â€¢ Tasks: Load â†’ FE â†’ Model â†’ Predict â†’ Save")
    print("   â€¢ Output: CSV + PNG reports")
    print("   â€¢ Orchestration: Prefect Tasks & Flows")
    
    if PREFECT_AVAILABLE:
        print("   â€¢ Prefect: âœ… Aktif")
    else:
        print("   â€¢ Prefect: âš ï¸  Mock mode")
    
    print("\nğŸ’¡ Production Deployment:")
    print("   1. prefect deployment build prefect_demand_forecast.py:demand_forecast_flow -n daily-forecast")
    print("   2. prefect deployment apply demand_forecast_flow-deployment.yaml") 
    print("   3. prefect agent start -q default")
    print("   4. Prefect UI: http://localhost:4200")
    
    print("\n" + "=" * 60)
    
    try:
        # Schedule setup (eÄŸitim amaÃ§lÄ±)
        deployment = setup_schedule()
        
        # Test Ã§alÄ±ÅŸmasÄ±
        print("\nğŸ§ª Test Ã§alÄ±ÅŸmasÄ± baÅŸlatÄ±lÄ±yor...")
        result = demand_forecast_flow(
            run_date=datetime.now().strftime('%Y-%m-%d'),
            forecast_days=7
        )
        
        print(f"\nâœ… Test baÅŸarÄ±lÄ±!")
        print(f"ğŸ“Š Status: {result['status']}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  KullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()