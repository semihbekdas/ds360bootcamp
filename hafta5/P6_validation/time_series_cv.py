#!/usr/bin/env python3
"""
Time Series Cross-Validation (Rolling-Origin) for LightGBM

Bu script zaman serisi iÃ§in uygun Ã§apraz doÄŸrulama yapar:
- Rolling-origin (expanding window) yaklaÅŸÄ±mÄ±
- 3 katlÄ± test ile robust performans Ã¶lÃ§Ã¼mÃ¼
- LightGBM pipeline'Ä± fonksiyonlaÅŸtÄ±rÄ±r

NEDEN SHUFFLE CV OLMAZ?
1. Temporal Leakage: Gelecek verisi ile geÃ§miÅŸ tahmin edilir (data leakage)
2. Pattern Bozukluk: Zaman baÄŸÄ±mlÄ± pattern'ler parÃ§alanÄ±r
3. GerÃ§ekÃ§i Olmama: Production'da shuffle yok, sadece geÃ§miÅŸ var

Rolling-Origin CV:
- Train window: Sabit veya geniÅŸleyen
- Valid window: 28 gÃ¼n (business horizon)
- 3 kat: 28, 56, 84 gÃ¼n Ã¶nceki validation

KullanÄ±m: python time_series_cv.py
"""

import pandas as pd
import numpy as np
import warnings
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any

# LightGBM
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    print("âŒ LightGBM kÃ¼tÃ¼phanesi bulunamadÄ±. 'pip install lightgbm' ile kurun.")
    LIGHTGBM_AVAILABLE = False

# Preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error

warnings.filterwarnings('ignore')

class TimeSeriesCV:
    """
    Time Series Cross-Validation sÄ±nÄ±fÄ±
    
    Rolling-origin validation ile zaman serisi iÃ§in uygun CV yapar.
    Temporal leakage'Ä± Ã¶nler ve gerÃ§ekÃ§i performans Ã¶lÃ§Ã¼mÃ¼ saÄŸlar.
    """
    
    def __init__(self, artifacts_path='./artifacts', validation_horizon=28, n_splits=3):
        self.artifacts_path = artifacts_path
        self.validation_horizon = validation_horizon
        self.n_splits = n_splits
        self.cv_results = []
        self.summary_metrics = {}
        
        # Reports klasÃ¶rÃ¼ oluÅŸtur
        os.makedirs(f'{artifacts_path}/reports', exist_ok=True)
        
        print("ğŸ“… Time Series Cross-Validation (Rolling-Origin)")
        print("âš ï¸  NEDEN SHUFFLE CV OLMAZ?")
        print("   1. Temporal Leakage: Gelecek verisi ile geÃ§miÅŸ tahmin edilir")
        print("   2. Pattern Bozukluk: Zaman baÄŸÄ±mlÄ± pattern'ler parÃ§alanÄ±r") 
        print("   3. GerÃ§ekÃ§i Olmama: Production'da shuffle yok, sadece geÃ§miÅŸ var")
        print("=" * 70)
    
    def load_data(self):
        """Feature engineered verileri yÃ¼kle"""
        
        print("\nğŸ“ 1. Veri yÃ¼kleniyor...")
        
        try:
            train_path = f'{self.artifacts_path}/datasets/fe_train.parquet'
            valid_path = f'{self.artifacts_path}/datasets/fe_valid.parquet'
            
            self.train_df = pd.read_parquet(train_path)
            self.valid_df = pd.read_parquet(valid_path)
            
            # BirleÅŸtir (tam zaman serisi iÃ§in)
            self.full_df = pd.concat([self.train_df, self.valid_df]).sort_index()
            
            print(f"   âœ“ Train: {self.train_df.shape}")
            print(f"   âœ“ Valid: {self.valid_df.shape}")
            print(f"   âœ“ Full: {self.full_df.shape}")
            print(f"   â€¢ Tarih aralÄ±ÄŸÄ±: {self.full_df.index.min()} - {self.full_df.index.max()}")
            print(f"   â€¢ ÃœrÃ¼n sayÄ±sÄ±: {self.full_df['item_id'].nunique()}")
            
        except FileNotFoundError as e:
            print(f"   âŒ Dosya bulunamadÄ±: {e}")
            raise
    
    def create_time_series_splits(self):
        """Rolling-origin CV split'leri oluÅŸtur"""
        
        print(f"\nğŸ“Š 2. {self.n_splits} katlÄ± time series splits oluÅŸturuluyor...")
        
        # TÃ¼m tarihleri al
        all_dates = sorted(self.full_df.index.unique())
        total_days = len(all_dates)
        
        print(f"   â€¢ Toplam gÃ¼n: {total_days}")
        print(f"   â€¢ Validation horizon: {self.validation_horizon} gÃ¼n")
        
        splits = []
        
        for fold in range(self.n_splits):
            # Validation son tarihini belirle
            # Fold 0: En son 28 gÃ¼n
            # Fold 1: Son 56-28 gÃ¼nler arasÄ± 
            # Fold 2: Son 84-56 gÃ¼nler arasÄ±
            
            val_end_offset = fold * self.validation_horizon
            val_start_offset = (fold + 1) * self.validation_horizon
            
            if val_start_offset >= total_days:
                print(f"   âš ï¸  Fold {fold}: Yeterli veri yok, atlanÄ±yor")
                continue
            
            # Tarih indeksleri
            val_end_idx = total_days - 1 - val_end_offset
            val_start_idx = total_days - val_start_offset
            
            # Train: BaÅŸlangÄ±Ã§tan validation baÅŸÄ±na kadar
            train_end_idx = val_start_idx - 1
            
            if train_end_idx < self.validation_horizon:  # Minimum train size
                print(f"   âš ï¸  Fold {fold}: Train Ã§ok kÃ¼Ã§Ã¼k, atlanÄ±yor")
                continue
            
            # Tarih aralÄ±klarÄ±
            train_dates = all_dates[:train_end_idx + 1]
            val_dates = all_dates[val_start_idx:val_end_idx + 1]
            
            split_info = {
                'fold': fold,
                'train_start': train_dates[0],
                'train_end': train_dates[-1],
                'val_start': val_dates[0],
                'val_end': val_dates[-1],
                'train_days': len(train_dates),
                'val_days': len(val_dates)
            }
            
            splits.append(split_info)
            
            print(f"   ğŸ“‹ Fold {fold}:")
            print(f"     â€¢ Train: {split_info['train_start']} - {split_info['train_end']} ({split_info['train_days']} gÃ¼n)")
            print(f"     â€¢ Valid: {split_info['val_start']} - {split_info['val_end']} ({split_info['val_days']} gÃ¼n)")
        
        self.splits = splits
        print(f"   âœ“ {len(splits)} geÃ§erli split oluÅŸturuldu")
        
        return splits
    
    def prepare_fold_data(self, split_info):
        """Bir fold iÃ§in veri hazÄ±rla"""
        
        # Train ve validation verilerini ayÄ±r
        train_mask = (self.full_df.index >= split_info['train_start']) & \
                     (self.full_df.index <= split_info['train_end'])
        val_mask = (self.full_df.index >= split_info['val_start']) & \
                   (self.full_df.index <= split_info['val_end'])
        
        fold_train = self.full_df[train_mask].copy()
        fold_val = self.full_df[val_mask].copy()
        
        return fold_train, fold_val
    
    def encode_categorical_features(self, train_df, val_df):
        """Kategorik Ã¶zellikleri encode et"""
        
        categorical_cols = ['item_id', 'store_id']
        label_encoders = {}
        
        for col in categorical_cols:
            if col in train_df.columns:
                le = LabelEncoder()
                
                # Train ile fit
                train_df[f'{col}_encoded'] = le.fit_transform(train_df[col])
                
                # Valid transform (unseen handling)
                val_encoded = []
                for val in val_df[col]:
                    if val in le.classes_:
                        val_encoded.append(le.transform([val])[0])
                    else:
                        val_encoded.append(0)  # Unseen = 0
                val_df[f'{col}_encoded'] = val_encoded
                
                label_encoders[col] = le
        
        return train_df, val_df, label_encoders
    
    def prepare_features_target(self, train_df, val_df):
        """Ã–zellik ve hedef deÄŸiÅŸkenleri hazÄ±rla"""
        
        # Ã–zellik sÃ¼tunlarÄ±
        lag_cols = [col for col in train_df.columns if 'lag_' in col]
        roll_cols = [col for col in train_df.columns if 'roll_' in col]
        date_cols = ['dow', 'dom', 'weekofyear', 'month']
        encoded_cols = [col for col in train_df.columns if '_encoded' in col]
        
        feature_cols = lag_cols + roll_cols + date_cols + encoded_cols
        target_col = 'sales'
        
        # Train
        X_train = train_df[feature_cols].fillna(0)
        y_train = train_df[target_col]
        
        # Valid
        X_val = val_df[feature_cols].fillna(0)
        y_val = val_df[target_col]
        
        return X_train, y_train, X_val, y_val, feature_cols
    
    def train_lightgbm_fold(self, X_train, y_train, X_val, y_val):
        """Bir fold iÃ§in LightGBM eÄŸit"""
        
        # LightGBM parametreleri
        lgb_params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42
        }
        
        # Dataset'ler
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        # Model eÄŸitimi
        model = lgb.train(
            lgb_params,
            train_data,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'valid'],
            num_boost_round=500,
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=0)  # Silent
            ]
        )
        
        return model
    
    def calculate_metrics(self, y_true, y_pred):
        """Metrikleri hesapla"""
        
        # Negatif deÄŸerleri 0 yap
        y_pred = np.maximum(y_pred, 0)
        
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        
        # MAPE
        mask = y_true != 0
        if mask.sum() > 0:
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = float('inf')
        
        # sMAPE
        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
        mask = denominator != 0
        if mask.sum() > 0:
            smape = np.mean(np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]) * 100
        else:
            smape = float('inf')
        
        return {
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape,
            'sMAPE': smape
        }
    
    def run_single_fold(self, split_info):
        """Tek fold Ã§alÄ±ÅŸtÄ±r"""
        
        fold = split_info['fold']
        print(f"   ğŸ”„ Fold {fold} Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # 1. Veri hazÄ±rlama
        fold_train, fold_val = self.prepare_fold_data(split_info)
        
        # 2. Encoding
        fold_train, fold_val, label_encoders = self.encode_categorical_features(fold_train, fold_val)
        
        # 3. Feature/target hazÄ±rlama
        X_train, y_train, X_val, y_val, feature_cols = self.prepare_features_target(fold_train, fold_val)
        
        print(f"     â€¢ Train: {X_train.shape}, Valid: {X_val.shape}")
        
        # 4. Model eÄŸitimi
        model = self.train_lightgbm_fold(X_train, y_train, X_val, y_val)
        
        # 5. Tahmin ve metrikler
        y_pred = model.predict(X_val, num_iteration=model.best_iteration)
        metrics = self.calculate_metrics(y_val.values, y_pred)
        
        print(f"     â€¢ MAE: {metrics['MAE']:.2f}, sMAPE: {metrics['sMAPE']:.2f}%")
        
        # SonuÃ§larÄ± kaydet
        fold_result = {
            'fold': fold,
            'split_info': split_info,
            'metrics': metrics,
            'model_info': {
                'best_iteration': model.best_iteration,
                'train_score': model.best_score['train']['rmse'],
                'valid_score': model.best_score['valid']['rmse']
            },
            'data_info': {
                'train_shape': list(X_train.shape),
                'valid_shape': list(X_val.shape),
                'n_items_train': fold_train['item_id'].nunique(),
                'n_items_valid': fold_val['item_id'].nunique()
            }
        }
        
        return fold_result
    
    def run_cross_validation(self):
        """Tam cross-validation Ã§alÄ±ÅŸtÄ±r"""
        
        print(f"\nğŸ”„ 3. {self.n_splits} katlÄ± cross-validation Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        self.cv_results = []
        
        for split_info in self.splits:
            try:
                fold_result = self.run_single_fold(split_info)
                self.cv_results.append(fold_result)
            except Exception as e:
                print(f"   âŒ Fold {split_info['fold']} hatasÄ±: {e}")
                continue
        
        print(f"   âœ“ {len(self.cv_results)} fold baÅŸarÄ±yla tamamlandÄ±")
    
    def aggregate_metrics(self):
        """Metrikleri topla ve ortalama al"""
        
        print(f"\nğŸ“Š 4. Metrikler toplanÄ±yor ve ortalama hesaplanÄ±yor...")
        
        if not self.cv_results:
            print("   âŒ HiÃ§ fold sonucu yok")
            return
        
        # Metrik listelerini topla
        metric_names = ['MAE', 'RMSE', 'MAPE', 'sMAPE']
        aggregated = {}
        
        for metric in metric_names:
            values = [result['metrics'][metric] for result in self.cv_results 
                     if result['metrics'][metric] != float('inf')]
            
            if values:
                aggregated[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'values': values
                }
            else:
                aggregated[metric] = {
                    'mean': float('inf'),
                    'std': 0,
                    'min': float('inf'),
                    'max': float('inf'),
                    'values': []
                }
        
        self.summary_metrics = aggregated
        
        # SonuÃ§larÄ± yazdÄ±r
        print(f"   ğŸ“ˆ CROSS-VALIDATION SONUÃ‡LARI ({len(self.cv_results)} fold):")
        print(f"   {'Metrik':<8} {'Ortalama':<10} {'Std':<8} {'Min':<8} {'Max':<8}")
        print(f"   {'-'*8} {'-'*10} {'-'*8} {'-'*8} {'-'*8}")
        
        for metric in metric_names:
            if aggregated[metric]['mean'] != float('inf'):
                print(f"   {metric:<8} {aggregated[metric]['mean']:<10.2f} "
                      f"{aggregated[metric]['std']:<8.2f} {aggregated[metric]['min']:<8.2f} "
                      f"{aggregated[metric]['max']:<8.2f}")
            else:
                print(f"   {metric:<8} {'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A':<8}")
        
        # Fold detaylarÄ±
        print(f"\n   ğŸ“‹ FOLD DETAYLARI:")
        for result in self.cv_results:
            fold = result['fold']
            mae = result['metrics']['MAE']
            smape = result['metrics']['sMAPE']
            train_days = result['split_info']['train_days']
            val_days = result['split_info']['val_days']
            print(f"   â€¢ Fold {fold}: MAE={mae:.2f}, sMAPE={smape:.2f}% "
                  f"(Train: {train_days}d, Valid: {val_days}d)")
    
    def generate_reports(self):
        """JSON ve Markdown raporlarÄ± oluÅŸtur"""
        
        print(f"\nğŸ“ 5. Raporlar oluÅŸturuluyor...")
        
        # JSON raporu
        json_report = {
            'experiment_info': {
                'method': 'Time Series Cross-Validation (Rolling-Origin)',
                'validation_horizon': self.validation_horizon,
                'n_splits_requested': self.n_splits,
                'n_splits_completed': len(self.cv_results),
                'timestamp': datetime.now().isoformat()
            },
            'methodology': {
                'approach': 'Rolling-Origin Validation',
                'why_not_shuffle': [
                    'Temporal Leakage: Gelecek verisi ile geÃ§miÅŸ tahmin edilir (data leakage)',
                    'Pattern Bozukluk: Zaman baÄŸÄ±mlÄ± pattern\'ler parÃ§alanÄ±r',
                    'GerÃ§ekÃ§i Olmama: Production\'da shuffle yok, sadece geÃ§miÅŸ var'
                ],
                'split_strategy': 'Expanding window train, fixed validation horizon'
            },
            'summary_metrics': self.summary_metrics,
            'fold_results': self.cv_results
        }
        
        json_path = f'{self.artifacts_path}/reports/tscv_report.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_report, f, indent=2, ensure_ascii=False, default=str)
        print(f"   âœ“ JSON raporu: {json_path}")
        
        # Markdown raporu
        md_content = self._generate_markdown_report()
        
        md_path = f'{self.artifacts_path}/reports/tscv_report.md'
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"   âœ“ Markdown raporu: {md_path}")
    
    def _generate_markdown_report(self):
        """Markdown raporu oluÅŸtur"""
        
        md = f"""# Time Series Cross-Validation Raporu

**Tarih:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Method:** Rolling-Origin Cross-Validation
**Model:** LightGBM Regressor

## ğŸ¯ AmaÃ§

Zaman serisi iÃ§in uygun Ã§apraz doÄŸrulama ile model performansÄ±nÄ± robust ÅŸekilde Ã¶lÃ§mek.

## âš ï¸ Neden Shuffle CV Olmaz?

1. **Temporal Leakage**: Gelecek verisi ile geÃ§miÅŸ tahmin edilir (data leakage)
2. **Pattern Bozukluk**: Zaman baÄŸÄ±mlÄ± pattern'ler parÃ§alanÄ±r
3. **GerÃ§ekÃ§i Olmama**: Production'da shuffle yok, sadece geÃ§miÅŸ var

## ğŸ“Š Cross-Validation YapÄ±sÄ±

- **Validation Horizon:** {self.validation_horizon} gÃ¼n
- **Toplam Fold:** {len(self.cv_results)} (baÅŸarÄ±lÄ±)
- **YaklaÅŸÄ±m:** Rolling-Origin (Expanding window train)

### Fold DetaylarÄ±

| Fold | Train BaÅŸlangÄ±Ã§ | Train BitiÅŸ | Valid BaÅŸlangÄ±Ã§ | Valid BitiÅŸ | Train GÃ¼n | Valid GÃ¼n |
|------|-----------------|-------------|-----------------|-------------|-----------|-----------|"""
        
        for result in self.cv_results:
            split_info = result['split_info']
            md += f"""
| {split_info['fold']} | {split_info['train_start'].strftime('%Y-%m-%d')} | {split_info['train_end'].strftime('%Y-%m-%d')} | {split_info['val_start'].strftime('%Y-%m-%d')} | {split_info['val_end'].strftime('%Y-%m-%d')} | {split_info['train_days']} | {split_info['val_days']} |"""
        
        md += f"""

## ğŸ“ˆ Performans SonuÃ§larÄ±

### Ã–zet Metrikler

| Metrik | Ortalama | Std Sapma | Min | Max |
|--------|----------|-----------|-----|-----|"""
        
        for metric_name, metric_data in self.summary_metrics.items():
            if metric_data['mean'] != float('inf'):
                md += f\"\"\"
| {metric_name} | {metric_data['mean']:.2f} | {metric_data['std']:.2f} | {metric_data['min']:.2f} | {metric_data['max']:.2f} |\"\"\"
            else:
                md += f\"\"\"
| {metric_name} | N/A | N/A | N/A | N/A |\"\"\"
        
        md += f\"\"\"

### Fold BazÄ±nda Detaylar

| Fold | MAE | RMSE | sMAPE (%) | Model Iterasyon |
|------|-----|------|-----------|-----------------|\"\"\"
        
        for result in self.cv_results:
            metrics = result['metrics']
            model_info = result['model_info']
            md += f\"\"\"
| {result['fold']} | {metrics['MAE']:.2f} | {metrics['RMSE']:.2f} | {metrics['sMAPE']:.2f} | {model_info['best_iteration']} |\"\"\"
        
        md += f\"\"\"

## ğŸ” Analiz ve Yorumlar

### Model TutarlÄ±lÄ±ÄŸÄ±
\"\"\"
        
        if len(self.cv_results) >= 2:
            smape_values = [r['metrics']['sMAPE'] for r in self.cv_results]
            smape_std = np.std(smape_values)
            
            if smape_std < 5:
                md += \"- **YÃ¼ksek tutarlÄ±lÄ±k**: sMAPE standart sapmasÄ± dÃ¼ÅŸÃ¼k ({:.2f}%)\\n\".format(smape_std)
            elif smape_std < 10:
                md += \"- **Orta tutarlÄ±lÄ±k**: sMAPE standart sapmasÄ± orta ({:.2f}%)\\n\".format(smape_std)
            else:
                md += \"- **DÃ¼ÅŸÃ¼k tutarlÄ±lÄ±k**: sMAPE standart sapmasÄ± yÃ¼ksek ({:.2f}%)\\n\".format(smape_std)
        
        # En iyi fold
        best_fold = min(self.cv_results, key=lambda x: x['metrics']['sMAPE'])
        worst_fold = max(self.cv_results, key=lambda x: x['metrics']['sMAPE'])
        
        md += f\"\"\"
- **En iyi fold**: Fold {best_fold['fold']} (sMAPE: {best_fold['metrics']['sMAPE']:.2f}%)
- **En kÃ¶tÃ¼ fold**: Fold {worst_fold['fold']} (sMAPE: {worst_fold['metrics']['sMAPE']:.2f}%)

### Production Ã–nerileri

1. **Model GÃ¼venilirliÄŸi**: CV sonuÃ§larÄ± model performansÄ±nÄ±n robust bir Ã¶lÃ§Ã¼mÃ¼nÃ¼ saÄŸlar
2. **Temporal Validation**: Rolling-origin yaklaÅŸÄ±mÄ± production senaryosunu yansÄ±tÄ±r
3. **Performance Beklentisi**: Ortalama sMAPE {self.summary_metrics['sMAPE']['mean']:.2f}% Â±{self.summary_metrics['sMAPE']['std']:.2f}%

### SÄ±nÄ±rlamalar

- Basit iteratif forecasting kullanÄ±ldÄ± (production iÃ§in iyileÅŸtirilebilir)
- Sadece LightGBM test edildi (ensemble modeller denenebilir)
- Sabit validation horizon (adaptive horizon test edilebilir)

---
*Bu rapor otomatik olarak oluÅŸturulmuÅŸtur - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
\"\"\"
        
        return md
    
    def run_full_pipeline(self):
        \"\"\"Tam time series CV pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r\"\"\"
        
        try:
            # 1. Veri yÃ¼kleme
            self.load_data()
            
            # 2. Split'leri oluÅŸtur
            self.create_time_series_splits()
            
            # 3. Cross-validation Ã§alÄ±ÅŸtÄ±r
            self.run_cross_validation()
            
            # 4. Metrikleri topla
            self.aggregate_metrics()
            
            # 5. RaporlarÄ± oluÅŸtur
            self.generate_reports()
            
            print(f\"\\nğŸ‰ Time Series Cross-Validation tamamlandÄ±!\")
            print(f\"ğŸ“Š {len(self.cv_results)} fold baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±\")
            if self.summary_metrics and 'sMAPE' in self.summary_metrics:
                print(f\"ğŸ“ˆ Ortalama sMAPE: {self.summary_metrics['sMAPE']['mean']:.2f}% Â±{self.summary_metrics['sMAPE']['std']:.2f}%\")
            print(f\"ğŸ“ Raporlar: {self.artifacts_path}/reports/\")
            
            return self.cv_results, self.summary_metrics
            
        except Exception as e:
            print(f\"\\nâŒ Time Series CV Pipeline hatasÄ±: {e}\")
            import traceback
            traceback.print_exc()
            raise

def main():
    \"\"\"Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu\"\"\"
    
    if not LIGHTGBM_AVAILABLE:
        print(\"âŒ LightGBM kÃ¼tÃ¼phanesi gerekli. 'pip install lightgbm' ile kurun.\")
        return
    
    print(\"=\" * 70)
    print(\"TIME SERIES CROSS-VALIDATION (ROLLING-ORIGIN)\")
    print(\"ğŸ¯ AmaÃ§: Zaman serisi iÃ§in uygun CV ile robust performans Ã¶lÃ§Ã¼mÃ¼\")
    print(\"=\" * 70)
    
    try:
        # Time Series CV'yi baÅŸlat
        tscv = TimeSeriesCV(validation_horizon=28, n_splits=3)
        
        # Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        cv_results, summary_metrics = tscv.run_full_pipeline()
        
        print(f\"\\nâœ… Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!\")
        print(f\"\\nğŸ’¡ Time Series CV FaydalarÄ±:\")
        print(f\"   â€¢ Temporal leakage'Ä± Ã¶nler\")
        print(f\"   â€¢ GerÃ§ekÃ§i performans Ã¶lÃ§Ã¼mÃ¼\")
        print(f\"   â€¢ Production senaryosunu yansÄ±tÄ±r\")
        print(f\"   â€¢ Model tutarlÄ±lÄ±ÄŸÄ±nÄ± test eder\")
        
    except KeyboardInterrupt:
        print(f\"\\nâ¹ï¸  KullanÄ±cÄ± tarafÄ±ndan durduruldu\")
    except Exception as e:
        print(f\"\\nğŸ’¥ Beklenmeyen hata: {e}\")

if __name__ == \"__main__\":
    main()